{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOnsH+h5OATVdIbIQmbk0qo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["##Importamos librerías"],"metadata":{"id":"9PyiD1OifrdM"}},{"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","import os\n","import time\n","!pip install tensorflow==2.15.1"],"metadata":{"id":"0lgiAC4Gfqzq","executionInfo":{"status":"ok","timestamp":1732628694407,"user_tz":180,"elapsed":10445,"user":{"displayName":"Julieta Texier","userId":"03239338560804844877"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"1b4d36cd-1519-41be-91b9-c8d07bedc5e6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tensorflow==2.15.1 in /usr/local/lib/python3.10/dist-packages (2.15.1)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (24.3.25)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (3.12.1)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (18.1.1)\n","Requirement already satisfied: ml-dtypes~=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (0.3.2)\n","Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (1.26.4)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (3.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (24.2)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (4.25.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (75.1.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (2.5.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (4.12.2)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (0.37.1)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (1.68.0)\n","Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (2.15.2)\n","Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (2.15.0)\n","Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.1) (2.15.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.1) (0.45.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (2.27.0)\n","Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (1.2.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (3.7)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (2.32.3)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (3.1.3)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (5.5.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (0.4.1)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (2024.8.30)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (3.0.2)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (0.6.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (3.2.2)\n"]}]},{"cell_type":"markdown","source":["Establecer GPU por defecto en caso de estar disponible."],"metadata":{"id":"vFia2mSohzMJ"}},{"cell_type":"code","source":["# Configurar para que TensorFlow utilice la GPU por defecto\n","gpus = tf.config.experimental.list_physical_devices('GPU')\n","if gpus:\n","    try:\n","        # Configurar para que TensorFlow asigne memoria dinámicamente\n","        for gpu in gpus:\n","            tf.config.experimental.set_memory_growth(gpu, True)\n","        # Especificar la GPU por defecto\n","        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n","        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n","    except RuntimeError as e:\n","        # Manejar error\n","        print(e)"],"metadata":{"id":"bh0cArDJhyA3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Preporcesamiento del texto"],"metadata":{"id":"dTUCDdX1LlrN"}},{"cell_type":"code","source":["path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n","text = open(path_to_file, 'rb').read().decode(encoding='utf-8')"],"metadata":{"id":"Dg7NdZi7lkzV","executionInfo":{"status":"ok","timestamp":1732628701605,"user_tz":180,"elapsed":543,"user":{"displayName":"Julieta Texier","userId":"03239338560804844877"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"7aec50f1-50b5-46ed-cbec-3dc8b877534e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n","1115394/1115394 [==============================] - 0s 0us/step\n"]}]},{"cell_type":"markdown","source":["Miramos las primeras filas del texto"],"metadata":{"id":"zIsfyMMXYhDD"}},{"cell_type":"code","source":["# Take a look at the first 250 characters in text\n","print(text[:250])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3L6SNXl5YacM","executionInfo":{"status":"ok","timestamp":1732628703138,"user_tz":180,"elapsed":411,"user":{"displayName":"Julieta Texier","userId":"03239338560804844877"}},"outputId":"db97af8a-a963-4747-9a45-b126ca2120f4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["First Citizen:\n","Before we proceed any further, hear me speak.\n","\n","All:\n","Speak, speak.\n","\n","First Citizen:\n","You are all resolved rather to die than to famish?\n","\n","All:\n","Resolved. resolved.\n","\n","First Citizen:\n","First, you know Caius Marcius is chief enemy to the people.\n","\n"]}]},{"cell_type":"markdown","source":["Vemos cuantos caracteres únicos tenemos"],"metadata":{"id":"aidaVE6D6iIu"}},{"cell_type":"code","source":["# The unique characters in the file\n","vocab = sorted(set(text))\n","print(f'{len(vocab)} unique characters')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RDz2NaLxYomF","executionInfo":{"status":"ok","timestamp":1732628705704,"user_tz":180,"elapsed":287,"user":{"displayName":"Julieta Texier","userId":"03239338560804844877"}},"outputId":"58e8c136-4445-43d2-c5ca-45d6fb3cca3e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["65 unique characters\n"]}]},{"cell_type":"markdown","source":["#Modelo caracter a caracter"],"metadata":{"id":"MlAjHzSmLvZT"}},{"cell_type":"markdown","source":["## Vectorizacion del texto"],"metadata":{"id":"vAcVAXPyoovh"}},{"cell_type":"markdown","source":["Previo al entrenamiento, necesitamos convertir el texto a una representacion numerica."],"metadata":{"id":"y8Ikwk--osnJ"}},{"cell_type":"markdown","source":["Creamos la capa tf.keras.layers.StringLookup convierte de tokens a IDs de caracteres:"],"metadata":{"id":"KLDTEeB1ZFsZ"}},{"cell_type":"code","source":["ids_from_chars = tf.keras.layers.StringLookup(\n","    vocabulary=list(vocab), mask_token=None)"],"metadata":{"id":"q2m056a3ZE6K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Dado que el proposito de este laboratorio es generar texto, tambien sera importante invertir esta representacion y recuperar texto legible desde estos IDs. Para esto podemos usar tf.keras.layers.StringLookup(..., invert=True)."],"metadata":{"id":"JO3PoAWsZZlz"}},{"cell_type":"code","source":["chars_from_ids = tf.keras.layers.StringLookup(\n","    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"],"metadata":{"id":"GCNawda4ZaaV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Finalmente usando tf.strings.reduce_join se pueden volver a juntar los caracteres en texto."],"metadata":{"id":"i__xy_5_aFXM"}},{"cell_type":"code","source":["def text_from_ids(ids):\n","  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"],"metadata":{"id":"Fl-hsSKvaKT3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Crear los ejemplos de entrenamiento"],"metadata":{"id":"ZkJCBeMNdds2"}},{"cell_type":"markdown","source":["\n","\n","Dividimos el texto en secuencias de ejemplo. Cada secuencia de entrada contendrá seq_length caracteres del texto.\n","\n","Para cada secuencia de entrada, los targets correspondientes contienen la misma longitud de texto, excepto que se desplazan un carácter hacia la derecha.\n","\n","Para hacer esto, usamos la función tf.data.Dataset.from_tensor_slices para convertir el vector de texto en una secuencia de índices de caracteres."],"metadata":{"id":"_qxIt45SdTRY"}},{"cell_type":"code","source":["all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n","all_ids"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qVxNugsSeIVB","executionInfo":{"status":"ok","timestamp":1732628714598,"user_tz":180,"elapsed":650,"user":{"displayName":"Julieta Texier","userId":"03239338560804844877"}},"outputId":"13dc5dc8-018d-4241-86f8-e40a6eeae2eb"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"],"metadata":{"id":"jZCVxXLWeLsu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for ids in ids_dataset.take(10):\n","    print(chars_from_ids(ids).numpy().decode('utf-8'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"umPkIJIheOPU","executionInfo":{"status":"ok","timestamp":1732628717480,"user_tz":180,"elapsed":276,"user":{"displayName":"Julieta Texier","userId":"03239338560804844877"}},"outputId":"dede25e8-1a1f-4e1b-a05a-48908177747a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["F\n","i\n","r\n","s\n","t\n"," \n","C\n","i\n","t\n","i\n"]}]},{"cell_type":"code","source":["seq_length = 100"],"metadata":{"id":"wRy0LuL8eVrP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["El método batch nos permite convertir fácilmente estos caracteres individuales en secuencias del tamaño deseado."],"metadata":{"id":"5jLZBlQJeaz8"}},{"cell_type":"code","source":["sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n","\n","for seq in sequences.take(1):\n","  print(chars_from_ids(seq))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9aTS1F66ebhX","executionInfo":{"status":"ok","timestamp":1732628722057,"user_tz":180,"elapsed":264,"user":{"displayName":"Julieta Texier","userId":"03239338560804844877"}},"outputId":"dce033e0-630b-494d-f12a-ae6e0f43dfb9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n"," b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n"," b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n"," b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n"," b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n"," b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n"," b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n"," b'o' b'u' b' '], shape=(101,), dtype=string)\n"]}]},{"cell_type":"markdown","source":["Es mas facil ver lo que esta haciendo si unimos de vuelta los tokens en texto:"],"metadata":{"id":"hiNI2TFcemlC"}},{"cell_type":"code","source":["for seq in sequences.take(5):\n","  print(text_from_ids(seq).numpy())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"esw6XOuVeoWr","executionInfo":{"status":"ok","timestamp":1732628724640,"user_tz":180,"elapsed":256,"user":{"displayName":"Julieta Texier","userId":"03239338560804844877"}},"outputId":"fc6644e5-018a-458d-88b9-13a94a47653c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n","b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n","b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n","b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n","b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"]}]},{"cell_type":"markdown","source":["Para el entrenamiento, necesitaremos un conjunto de datos de pares (input, label). Donde input y label son secuencias. En cada timestep, la entrada es el carácter actual y la etiqueta es el siguiente carácter.\n","\n","Aquí hay una función que toma una secuencia como entrada, la duplica y la desplaza para alinear la entrada y la etiqueta para cada timestep:"],"metadata":{"id":"WqmWlD81hNa3"}},{"cell_type":"code","source":["def split_input_target(sequence):\n","    input_text = sequence[:-1]\n","    target_text = sequence[1:]\n","    return input_text, target_text"],"metadata":{"id":"TTrqyh0MhPvb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = sequences.map(split_input_target)"],"metadata":{"id":"xPk7jRC7hUoD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for input_example, target_example in dataset.take(1):\n","    print(\"Input :\", text_from_ids(input_example).numpy())\n","    print(\"Target:\", text_from_ids(target_example).numpy())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RH4P3jNBhX2l","executionInfo":{"status":"ok","timestamp":1732628733146,"user_tz":180,"elapsed":5,"user":{"displayName":"Julieta Texier","userId":"03239338560804844877"}},"outputId":"ebd2b29b-0f74-4dec-a9c1-4ba153bec442"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n","Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"]}]},{"cell_type":"markdown","source":["### Batches de entrenamiento\n","\n","Usamos `tf.data` para dividir el texto en secuencias manejables. Pero antes de introducir estos datos en el modelo, es necesario mezclarlos y batchearlos."],"metadata":{"id":"GnaM9hcViJ0M"}},{"cell_type":"code","source":["# Batch size\n","BATCH_SIZE = 64\n","\n","# Buffer size to shuffle the dataset\n","# (TF data is designed to work with possibly infinite sequences,\n","# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n","# it maintains a buffer in which it shuffles elements).\n","BUFFER_SIZE = 10000\n","\n","dataset = (\n","    dataset\n","    .shuffle(BUFFER_SIZE)\n","    .batch(BATCH_SIZE, drop_remainder=True)\n","    .prefetch(tf.data.experimental.AUTOTUNE))\n","\n","dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ntB2AtGXjrwM","executionInfo":{"status":"ok","timestamp":1732628738990,"user_tz":180,"elapsed":383,"user":{"displayName":"Julieta Texier","userId":"03239338560804844877"}},"outputId":"29260799-849e-400c-b9c4-027b445d1d45"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","source":["## Construccion del modelo"],"metadata":{"id":"0ZihC69sk3E_"}},{"cell_type":"markdown","source":["En esta sección definimos el modelo como una subclase de `keras.Model`\n","\n","Este modelo tiene tres capas:\n","\n","* `tf.keras.layers.Embedding`: La capa de entrada. Una lookup table entrenable que asignará cada ID de carácter a un vector con dimensiones `embedding_dim`;\n","* `tf.keras.layers.GRU`: una capa recurrente GRU de tamaño `units=rnn_units` (también se puede usar una capa LSTM aquí).\n","* `tf.keras.layers.Dense`: La capa de salida, con salidas `vocab_size`. Genera un logit para cada carácter del vocabulario. Estas son las probabilidades de cada caracter según el modelo."],"metadata":{"id":"heAxi-J8k4c3"}},{"cell_type":"code","source":["# Length of the vocabulary in StringLookup Layer\n","vocab_size = len(ids_from_chars.get_vocabulary())\n","\n","# The embedding dimension\n","embedding_dim = 128\n","\n","# Number of RNN units\n","rnn_units = 512"],"metadata":{"id":"LSswUiVBk37L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MyModel(tf.keras.Model):\n","  def __init__(self, vocab_size, embedding_dim, rnn_units):\n","    super().__init__()\n","    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","    self.gru = tf.keras.layers.GRU(rnn_units,\n","                                   return_sequences=True,\n","                                   return_state=True)\n","    self.dense = tf.keras.layers.Dense(vocab_size)\n","\n","  def call(self, inputs, states=None, return_state=False, training=False):\n","    x = inputs\n","    x = self.embedding(x)\n","    if states is None:\n","      states = self.gru.get_initial_state(x)\n","    x, states = self.gru(x, initial_state=states, training=training)\n","    x = self.dense(x, training=training)\n","\n","    if return_state:\n","      return x, states\n","    else:\n","      return x"],"metadata":{"id":"Mz4FGA1vlC0m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = MyModel(\n","    vocab_size=vocab_size,\n","    embedding_dim=embedding_dim,\n","    rnn_units=rnn_units)"],"metadata":{"id":"bZ-PPFdVlJ8b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Por cada caracter el modelo calcula su embedding, corre la GRU un timestep con el embedding como entrada y aplica la capa densa para generar los logits prediciendo la probabilidades del siguiente caracter."],"metadata":{"id":"Lkv_dST0l9FX"}},{"cell_type":"markdown","source":["## Probar el modelo\n","\n","Ejecutamos el modelo para ver que se comporta como se esperaba.\n","\n","Primero verificamos la shape de salida:"],"metadata":{"id":"EW7i-nqMouxy"}},{"cell_type":"code","source":["for input_example_batch, target_example_batch in dataset.take(1):\n","    example_batch_predictions = model(input_example_batch)\n","    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GD6T0_VjozL2","executionInfo":{"status":"ok","timestamp":1732628766345,"user_tz":180,"elapsed":5946,"user":{"displayName":"Julieta Texier","userId":"03239338560804844877"}},"outputId":"a5909ca1-0de4-4a77-aa5a-ac76982402e2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"]}]},{"cell_type":"markdown","source":["En el ejemplo anterior, la longitud de la secuencia de la entrada es 100, pero el modelo se puede ejecutar con entradas de cualquier longitud:"],"metadata":{"id":"hNEWd96Ao1fZ"}},{"cell_type":"code","source":["model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6-dFKPMIo3OO","executionInfo":{"status":"ok","timestamp":1732628786990,"user_tz":180,"elapsed":274,"user":{"displayName":"Julieta Texier","userId":"03239338560804844877"}},"outputId":"75e346e1-30b2-4503-a6fc-b0a565cf3aaa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"my_model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       multiple                  8448      \n","                                                                 \n"," gru (GRU)                   multiple                  986112    \n","                                                                 \n"," dense (Dense)               multiple                  33858     \n","                                                                 \n","=================================================================\n","Total params: 1028418 (3.92 MB)\n","Trainable params: 1028418 (3.92 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["Para obtener predicciones reales del modelo, se deben tomar muestras de la distribución de salida para obtener índices de caracteres reales. Esta distribución está definida por los logits sobre el vocabulario de los caracteres.\n","\n","Nota: Es importante tomar una muestra de esta distribución, ya que tomar el argmax de la distribución puede fácilmente hacer que el modelo se atasque en un bucle.\n","\n","Tomando como ejemplo el primero del batch:"],"metadata":{"id":"ZVSB6Dwcvo4l"}},{"cell_type":"code","source":["sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n","sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"],"metadata":{"id":"egisEIU9vZF_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Esto nos da para cada timestep una predicción del siguiente índice de caracteres:"],"metadata":{"id":"oUy5Lb4zvsOT"}},{"cell_type":"code","source":["sampled_indices"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UHC3jLpbvwqT","executionInfo":{"status":"ok","timestamp":1732628806053,"user_tz":180,"elapsed":279,"user":{"displayName":"Julieta Texier","userId":"03239338560804844877"}},"outputId":"aa4d5fde-7964-448e-d46a-6a3886fc58f1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([53, 55, 30, 25, 63, 47, 46, 32, 51, 55, 30, 40, 35,  5, 35, 13, 27,\n","        0, 11, 49, 63, 42, 32, 41,  9, 49, 53, 16, 51, 27, 15, 46, 49, 31,\n","       18,  4, 12, 61, 12, 30, 12, 33, 54, 23,  3, 39, 26, 46, 15, 46, 14,\n","       40, 15, 26, 12, 64,  9, 13, 39, 16, 32, 13,  2, 13, 46, 17, 36, 15,\n","       36,  9, 27, 50, 11, 42, 24,  7, 43, 58, 15, 19, 37, 48, 11, 29, 34,\n","       47, 40, 46, 36, 19, 26, 53, 33, 16,  4, 29, 14,  5, 55, 11])"]},"metadata":{},"execution_count":25}]},{"cell_type":"markdown","source":["Por ultimo los decodificamos para ver el texto predicho por este modelo no entrenado:"],"metadata":{"id":"gusDV4Rpv6XE"}},{"cell_type":"code","source":["print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n","print()\n","print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9jKAeqp7v7Gw","executionInfo":{"status":"ok","timestamp":1732628808607,"user_tz":180,"elapsed":257,"user":{"displayName":"Julieta Texier","userId":"03239338560804844877"}},"outputId":"ab98c93a-95cf-4b24-8fef-ddad2c64d2cd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Input:\n"," b\",\\nStirr'd up by God, thus boldly for his king:\\nMy Lord of Hereford here, whom you call king,\\nIs a fo\"\n","\n","Next Char Predictions:\n"," b'npQLxhgSlpQaV&V?N[UNK]:jxcSb.jnClNBgjRE$;v;Q;ToJ!ZMgBgAaBM;y.?ZCS? ?gDWBW.Nk:cK,dsBFXi:PUhagWFMnTC$PA&p:'\n"]}]},{"cell_type":"markdown","source":["Como vemos, la prediccion sin entrenar el modelo es mala"],"metadata":{"id":"8EtIYuGg31K5"}},{"cell_type":"markdown","source":["## Entrenamiento del modelo"],"metadata":{"id":"6WzNi5vRWwZt"}},{"cell_type":"markdown","source":["El problema puede tratarse como un problema de clasificación estándar. Dado el estado RNN anterior y la entrada en este timestep, predice la clase del siguiente carácter."],"metadata":{"id":"4zYbK2gcWxUh"}},{"cell_type":"markdown","source":["### Agregamos un optimizador y una funcion costo"],"metadata":{"id":"KunDodtnW7qZ"}},{"cell_type":"markdown","source":["La función de pérdida estándar `tf.keras.losses.sparse_categorical_crossentropy` funciona en este caso porque se aplica en la última dimensión de las predicciones.\n","\n","Debido a que su modelo devuelve logits, necesita configurar el indicador `from_logits`."],"metadata":{"id":"1jLR20p0W-Ci"}},{"cell_type":"code","source":["loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"],"metadata":{"id":"a6eFZNEXXA-P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n","print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n","print(\"Mean loss:        \", example_batch_mean_loss)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OweIHyfWXFsn","executionInfo":{"status":"ok","timestamp":1732628818783,"user_tz":180,"elapsed":242,"user":{"displayName":"Julieta Texier","userId":"03239338560804844877"}},"outputId":"1054d3b2-7fac-4ef0-907f-2f29655c434a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n","Mean loss:         tf.Tensor(4.190164, shape=(), dtype=float32)\n"]}]},{"cell_type":"markdown","source":["Un modelo recién inicializado no debería estar demasiado seguro de sí mismo, todos los logits de salida deberían tener magnitudes similares. Para confirmar esto, puede comprobar que la exponencial del costo medio es aproximadamente igual al tamaño del vocabulario. Una pérdida mucho mayor significa que el modelo está seguro de sus respuestas incorrectas y está mal inicializado:"],"metadata":{"id":"z3S2eySBXcOC"}},{"cell_type":"code","source":["tf.exp(example_batch_mean_loss).numpy()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tyoZpM65XdAs","executionInfo":{"status":"ok","timestamp":1732628820945,"user_tz":180,"elapsed":255,"user":{"displayName":"Julieta Texier","userId":"03239338560804844877"}},"outputId":"ac670f64-e8f0-4117-893c-89e14ef4ca28"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["66.03362"]},"metadata":{},"execution_count":29}]},{"cell_type":"markdown","source":["Como podemos comprobar la exponencial de costo es similar al tamaño de caracteres (65)"],"metadata":{"id":"8C6cWG8yXkZF"}},{"cell_type":"markdown","source":["Compilamos el modelo con tf.keras.Model.compile indicando el optimizador y la funcion costo:"],"metadata":{"id":"wEfW3HqXXvYT"}},{"cell_type":"code","source":["model.compile(optimizer='adam', loss=loss)"],"metadata":{"id":"Sn8-2TcyXy4e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Checkpoints del modelo"],"metadata":{"id":"iy0RHzdsX3VD"}},{"cell_type":"markdown","source":["Usamos el callback `tf.keras.callbacks.ModelCheckpoint` para que se guarden checkpoints del modelo durante el entrenamiento."],"metadata":{"id":"7dqzFXwIX30R"}},{"cell_type":"code","source":["# Directory where the checkpoints will be saved\n","checkpoint_dir = './training_checkpoints'\n","# Name of the checkpoint files\n","checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}.weights.h5\")\n","\n","checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_prefix,\n","    save_weights_only=True)"],"metadata":{"id":"27vqSpvqX8Bq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Ejecucion del entrenamiento"],"metadata":{"id":"xiXIApobZQ__"}},{"cell_type":"code","source":["EPOCHS = 30"],"metadata":{"id":"uhQ-zPoNZWET"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X-zzGx1TZYJq","executionInfo":{"status":"ok","timestamp":1732636442769,"user_tz":180,"elapsed":7591196,"user":{"displayName":"Julieta Texier","userId":"03239338560804844877"}},"outputId":"9bfa8fc8-9cb4-4c68-d34d-a6406f58906b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/30\n","172/172 [==============================] - 229s 1s/step - loss: 2.9749\n","Epoch 2/30\n","172/172 [==============================] - 217s 1s/step - loss: 2.2095\n","Epoch 3/30\n","172/172 [==============================] - 223s 1s/step - loss: 1.9450\n","Epoch 4/30\n","172/172 [==============================] - 231s 1s/step - loss: 1.7549\n","Epoch 5/30\n","172/172 [==============================] - 219s 1s/step - loss: 1.6274\n","Epoch 6/30\n","172/172 [==============================] - 221s 1s/step - loss: 1.5413\n","Epoch 7/30\n","172/172 [==============================] - 220s 1s/step - loss: 1.4797\n","Epoch 8/30\n","172/172 [==============================] - 222s 1s/step - loss: 1.4334\n","Epoch 9/30\n","172/172 [==============================] - 227s 1s/step - loss: 1.3960\n","Epoch 10/30\n","172/172 [==============================] - 220s 1s/step - loss: 1.3656\n","Epoch 11/30\n","172/172 [==============================] - 220s 1s/step - loss: 1.3388\n","Epoch 12/30\n","172/172 [==============================] - 219s 1s/step - loss: 1.3156\n","Epoch 13/30\n","172/172 [==============================] - 219s 1s/step - loss: 1.2946\n","Epoch 14/30\n","172/172 [==============================] - 225s 1s/step - loss: 1.2747\n","Epoch 15/30\n","172/172 [==============================] - 221s 1s/step - loss: 1.2560\n","Epoch 16/30\n","172/172 [==============================] - 218s 1s/step - loss: 1.2383\n","Epoch 17/30\n","172/172 [==============================] - 221s 1s/step - loss: 1.2211\n","Epoch 18/30\n","172/172 [==============================] - 224s 1s/step - loss: 1.2042\n","Epoch 19/30\n","172/172 [==============================] - 221s 1s/step - loss: 1.1874\n","Epoch 20/30\n","172/172 [==============================] - 221s 1s/step - loss: 1.1712\n","Epoch 21/30\n","172/172 [==============================] - 221s 1s/step - loss: 1.1554\n","Epoch 22/30\n","172/172 [==============================] - 220s 1s/step - loss: 1.1377\n","Epoch 23/30\n","172/172 [==============================] - 221s 1s/step - loss: 1.1212\n","Epoch 24/30\n","172/172 [==============================] - 222s 1s/step - loss: 1.1054\n","Epoch 25/30\n","172/172 [==============================] - 220s 1s/step - loss: 1.0884\n","Epoch 26/30\n","172/172 [==============================] - 224s 1s/step - loss: 1.0720\n","Epoch 27/30\n","172/172 [==============================] - 235s 1s/step - loss: 1.0550\n","Epoch 28/30\n","172/172 [==============================] - 234s 1s/step - loss: 1.0388\n","Epoch 29/30\n","172/172 [==============================] - 232s 1s/step - loss: 1.0226\n","Epoch 30/30\n","172/172 [==============================] - 224s 1s/step - loss: 1.0069\n"]}]},{"cell_type":"markdown","source":["## Generacion de texto"],"metadata":{"id":"nKdrZw80v3Sq"}},{"cell_type":"markdown","source":["La forma más sencilla de generar texto con este modelo es ejecutarlo en un bucle y realizar un seguimiento del estado interno del modelo a medida que lo ejecutamos.\n","\n","Para generar texto, la salida del modelo se retroalimenta a la entrada\n","\n","Cada vez que llamamos al modelo, pasamos algún texto y un estado interno. El modelo devuelve una predicción para el siguiente caracter y su nuevo estado. Vuelva a pasar la predicción y el estado para continuar generando texto."],"metadata":{"id":"ZQfSASTnv3_Z"}},{"cell_type":"markdown","source":["Lo siguiente hace una predicción de un solo paso:"],"metadata":{"id":"YDuc4ujpv868"}},{"cell_type":"code","source":["class OneStep(tf.keras.Model):\n","  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n","    super().__init__()\n","    self.temperature = temperature\n","    self.model = model\n","    self.chars_from_ids = chars_from_ids\n","    self.ids_from_chars = ids_from_chars\n","\n","    # Create a mask to prevent \"[UNK]\" from being generated.\n","    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n","    sparse_mask = tf.SparseTensor(\n","        # Put a -inf at each bad index.\n","        values=[-float('inf')]*len(skip_ids),\n","        indices=skip_ids,\n","        # Match the shape to the vocabulary\n","        dense_shape=[len(ids_from_chars.get_vocabulary())])\n","    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n","\n","  @tf.function\n","  def generate_one_step(self, inputs, states=None):\n","    # Convert strings to token IDs.\n","    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n","    input_ids = self.ids_from_chars(input_chars).to_tensor()\n","\n","    # Run the model.\n","    # predicted_logits.shape is [batch, char, next_char_logits]\n","    predicted_logits, states = self.model(inputs=input_ids, states=states,\n","                                          return_state=True)\n","    # Only use the last prediction.\n","    predicted_logits = predicted_logits[:, -1, :]\n","    predicted_logits = predicted_logits/self.temperature\n","    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n","    predicted_logits = predicted_logits + self.prediction_mask\n","\n","    # Sample the output logits to generate token IDs.\n","    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n","    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n","\n","    # Convert from token ids to characters\n","    predicted_chars = self.chars_from_ids(predicted_ids)\n","\n","    # Return the characters and model state.\n","    return predicted_chars, states"],"metadata":{"id":"I2X9vW1MwAuD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"],"metadata":{"id":"Dba1-0XmwFBd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Lo ejecutamos en un bucle para generar texto. Al observar el texto generado, veremos que el modelo sabe cuándo poner mayúsculas, hacer párrafos e imita un vocabulario de escritura similar a sheakspeare. Probamos con 10, 20 y 30 epocas y pudimos ver que a medida que aumentabamos las epocas las frases van siendo mas coherentes."],"metadata":{"id":"B2efxlggwRwR"}},{"cell_type":"code","source":["start = time.time()\n","states = None\n","next_char = tf.constant(['First Citizen'])\n","result = [next_char]\n","\n","for n in range(300):\n","  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n","  result.append(next_char)\n","\n","result = tf.strings.join(result)\n","end = time.time()\n","print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n","print('\\nRun time:', end - start)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hwrrPEqjwUdt","executionInfo":{"status":"ok","timestamp":1732639251092,"user_tz":180,"elapsed":1063,"user":{"displayName":"Julieta Texier","userId":"03239338560804844877"}},"outputId":"102ba63b-18eb-4fa1-a234-5a32ceeb623d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["First Citizen:\n","But how fares your son's, being something uptorsty?\n","Clarence! My drightly your conceit; it\n","returns the marice of thy tears, which waiting from the peapes company\n","Ressenged on our most dignifies demands;\n","And strike after rume of him that thou art\n","As Paese: you have recooted me for my most shall be  \n","\n","________________________________________________________________________________\n","\n","Run time: 0.804802417755127\n"]}]},{"cell_type":"markdown","source":["generamos 4 frases mas para comparar"],"metadata":{"id":"sP4I1skdVxPb"}},{"cell_type":"code","source":["start = time.time()\n","states = None\n","next_char = tf.constant(['First Citizen'])\n","result = [next_char]\n","\n","for n in range(300):\n","  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n","  result.append(next_char)\n","\n","result = tf.strings.join(result)\n","end = time.time()\n","print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n","print('\\nRun time:', end - start)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QQdAbxvGVeok","executionInfo":{"status":"ok","timestamp":1732639215701,"user_tz":180,"elapsed":1591,"user":{"displayName":"Julieta Texier","userId":"03239338560804844877"}},"outputId":"83084f89-2752-4c87-9e12-5d4c46fb159b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["First Citizen:\n","For ever-hole a thousand officers: at my hands\n","While vesing cut of monstrous misant, be so; O, then\n","I know no hope.\n","\n","PRINCE EDWARD:\n","All shall thy want of sto'l hollow forsook in him?\n","Teach this case I have heard a slagest man\n","Than blood which no pill'd your forsy; set A brace and borough!\n","\n","SICINIU \n","\n","________________________________________________________________________________\n","\n","Run time: 1.3600642681121826\n"]}]},{"cell_type":"code","source":["start = time.time()\n","states = None\n","next_char = tf.constant(['First Citizen'])\n","result = [next_char]\n","\n","for n in range(400):\n","  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n","  result.append(next_char)\n","\n","result = tf.strings.join(result)\n","end = time.time()\n","print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n","print('\\nRun time:', end - start)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rwQp2lCuWQXB","executionInfo":{"status":"ok","timestamp":1732639294915,"user_tz":180,"elapsed":925,"user":{"displayName":"Julieta Texier","userId":"03239338560804844877"}},"outputId":"e8f4c953-8b96-4561-9dc6-20f46a7cff52"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["First Citizen:\n","We'll Prospe in the marketh and his lands with down:\n","Then shall I near again, too late 'sil\n","him not: I will attend me, dost thou did make us the\n","farvot: with a lengy of the title of all discretits,\n","Because honourable strew thy blood: they are traitor,\n","The commonts of the two?\n","\n","BOHNA:\n","'Tis gone we swear it, put in the chair of your followers:\n","Nay, patience, hear our love,\n","Shortly I throw my fathe \n","\n","________________________________________________________________________________\n","\n","Run time: 0.679373025894165\n"]}]},{"cell_type":"code","source":["start = time.time()\n","states = None\n","next_char = tf.constant(['JULIET'])\n","result = [next_char]\n","\n","for n in range(500):\n","  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n","  result.append(next_char)\n","\n","result = tf.strings.join(result)\n","end = time.time()\n","print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n","print('\\nRun time:', end - start)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n5sXgqSwWQLO","executionInfo":{"status":"ok","timestamp":1732639536100,"user_tz":180,"elapsed":1374,"user":{"displayName":"Julieta Texier","userId":"03239338560804844877"}},"outputId":"8327858f-b46e-4a91-ed04-d40576df8768"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["JULIET:\n","O BoyoD Worth thou palth-may.\n","Know, Mard;--O, what nurisb-rew meez,\n","Put moliniat,\n","God and Lond guilton me a duel at,\n","Sying' peer;\n","You'll dear so founger3 PRyISCLue'lls.\n","\n","PROSPERO:\n","Appime\n","of Romes jointenhian.\n","\n","Amblibro, believe not inctriasute turror,\n","Haves and bjop kispstancledey caquque when\n","achinabit\n","Nating-eaticoble. Balkfolds bys;\n","For plugapup? therebike\n","Losis Buhberie!\n","To have I voly Tybalt. Grorn low:\n","The Earl of Mercupe widow? Dare our retort?\n","Arm'Od looket Minen:\n","Therefoldwamed, up co \n","\n","________________________________________________________________________________\n","\n","Run time: 1.1021220684051514\n"]}]},{"cell_type":"code","source":["start = time.time()\n","states = None\n","next_char = tf.constant(['First Citizen'])\n","result = [next_char]\n","\n","for n in range(800):\n","  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n","  result.append(next_char)\n","\n","result = tf.strings.join(result)\n","end = time.time()\n","print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n","print('\\nRun time:', end - start)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nJndkkcPWgJ8","executionInfo":{"status":"ok","timestamp":1732639338878,"user_tz":180,"elapsed":2696,"user":{"displayName":"Julieta Texier","userId":"03239338560804844877"}},"outputId":"c4b77257-7a15-4b20-e63c-7194869d2726"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["First Citizen:\n","I have a happiness, I will dram her eye,\n","Came to your king'd and wedding chestion\n","And metries lirth than you upon by.\n","\n","DUKE OF AUMERLE:\n","My lord, your queen poherty, very seove, I would pardon me;\n","No, I'll not will keep and think our faithful self-door;\n","You shall be done in Carisaunt flatker me in myself?\n","\n","HORTENSIO:\n","Madam, my lord; not to know his sister\n","Servant was to be so; then be mad.\n","\n","MONTAGUE:\n","Worth this, Sir, mumber not! I will grant? upon\n","him!\n","\n","Shepherd:\n","Look to the Capitol?\n","\n","LIONT:\n","Ristlef Gloucester, to her faults with mindred\n","Alas, I know you are a sort such butters of the issue.\n","\n","KING HENRY VI:\n","First son: and friar, being slender we have seen.\n","\n","Pedant:\n","O, what I'll not be.\n","\n","JULIET:\n","Beseech you, like a falsehood of the death.\n","Now, Trubtiness, sir? how sweet best tofful soul is \n","\n","________________________________________________________________________________\n","\n","Run time: 2.423008680343628\n"]}]},{"cell_type":"markdown","source":["A medida que vamos cambiando el largo de la secuencia, vemos que logra armar un dialogo en el formato de verso y prosa que requerimos. Si le cambiamos la palabra de origen continúa armando textos coherentes y con el formato de verso y prosa que requerimos."],"metadata":{"id":"NiNQ0gSPWfhe"}},{"cell_type":"markdown","source":["Ese modelo tenía una temperatura igual a 1, ahora vamos a probar con distintas temperaturas para ver que textos devuelve."],"metadata":{"id":"67kHdlMphhUv"}},{"cell_type":"code","source":["one_step_model = OneStep(model, chars_from_ids, ids_from_chars, temperature = 0.5)"],"metadata":{"id":"aG-s5alMhoOA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["start = time.time()\n","states = None\n","next_char = tf.constant(['First Citizen'])\n","result = [next_char]\n","\n","for n in range(1000):\n","  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n","  result.append(next_char)\n","\n","result = tf.strings.join(result)\n","end = time.time()\n","print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n","print('\\nRun time:', end - start)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gjCeguWFhw2j","executionInfo":{"status":"ok","timestamp":1732639499623,"user_tz":180,"elapsed":1762,"user":{"displayName":"Julieta Texier","userId":"03239338560804844877"}},"outputId":"ccf9a9b0-19c2-4961-fcea-af1e0c14bc19"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["First Citizen:\n","He'la,\n","do, dam''d by\n","Rikely apides,\n","Blufs too toun is butied: winterous Eunher bid le.\n","\n","ATH:\n","And yet me pows; I told you what: what's flautisp;\n","Besids' you, dracI.\n",": it custom aburue yel? why igly should!\n","\n","TRANIO:\n","What teked was; l'dein,\n","A\n","meeting natide, agrwerateden jaw.\n","Who wills, 'tisg!\n","Say,' faces you. At just,\n","Weg-done: noino, telk not.--\n","ANNELO:\n","O bravid, virtuzo! Juliegh!'\n","OH No?\n","\n","ISABELLA:\n","comfene and hapits mut, weswifegefol; nod brings afted\n","off:\n","Let's hief man try upperGit Vellow:\n","Didmfur!\n","'Twill mirdlery I; no embtality, a\n","blieh--'onqu'k, juck miso--tuills zaulty precipe:\n","Briargs and when Kame.\n","\n","Second SuT'RLVAK:\n","His.\n","First Sebain a JulomityCUTY:\n","\n","BENCOUrse:\n","Pittlus\n","Make not 'Widl-\n","Bitionfly,'\n","Curr he, him fray:\n","Quest Laiches; fhom Plince bl comon'st losp, 'both 'Ssalt:\n","I' ta'tty thousands. Was Af-same;\n","West I'Bqueatily wenh amVO Dukibs;\n","Yea, impudey, of what nam-shoulder'sing runifp, noke\n","sycalate\n","Till juddes upon, I;' mis-shanero to't;\n","like lie,\n","fohs, is it your wall;w \n","\n","________________________________________________________________________________\n","\n","Run time: 1.4588162899017334\n"]}]},{"cell_type":"markdown","source":["No vemos demasiados cambios poniendo la temperatura en 0.5, probemos con algo mas bajo todavía"],"metadata":{"id":"7Sw2Lm_mh7-k"}},{"cell_type":"code","source":["one_step_model = OneStep(model, chars_from_ids, ids_from_chars, temperature = 0.1)"],"metadata":{"id":"P0OJOPmMiB_s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["start = time.time()\n","states = None\n","next_char = tf.constant(['First Citizen'])\n","result = [next_char]\n","\n","for n in range(1000):\n","  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n","  result.append(next_char)\n","\n","result = tf.strings.join(result)\n","end = time.time()\n","print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n","print('\\nRun time:', end - start)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i7PgW6GViJWH","executionInfo":{"status":"ok","timestamp":1732639417528,"user_tz":180,"elapsed":2390,"user":{"displayName":"Julieta Texier","userId":"03239338560804844877"}},"outputId":"60a3b9b9-81c4-41e3-8edb-fea695d49907"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["First Citizen:\n","We are too soon, and speak the sea, having no more\n","Than the sun short a poor prince, and still strike at all.\n","\n","BUCKINGHAM:\n","What is the marriage of the county slay;\n","For the law shall not be all the seasons.\n","\n","First Servingman:\n","What news abroad?\n","\n","BRUTUS:\n","We are all unpossess it to the crown, the senate bods,\n","That we may command a feast of the world:\n","And when he says it is to be a soul, and therefore,\n","I will be set of his looks are speeding soul!\n","And what the duke is spite of all the duke.\n","\n","GLOUCESTER:\n","And therefore for a heavy sound,\n","And therefore leave us to the crown,\n","And with the state and prince as thou art deceived;\n","And therefore let us so he did.\n","\n","DUKE OF YORK:\n","What is the county serve?\n","\n","Second Servingman:\n","What news abroad?\n","\n","BRUTUS:\n","We are all undertake to see the county of the county\n","Part of the seasing manner of my soul is worth\n","Than when they say, and there a man doth so,\n","That we may come to the crown, and so we proclaim\n","What you have spent an enemy,\n","And so I come, sir, and we  \n","\n","________________________________________________________________________________\n","\n","Run time: 2.283478260040283\n"]}]},{"cell_type":"markdown","source":["Tampoco vemos cambios en el fromato del texto y en la coherencia, porque al poner una temperatura baja acercamos las probabilidades altas a 1 y las bajas a 0 entonces hacemos que las palabras que tenian mas probabilidades de seleccionarse sigan siendo las mas seleccionadas."],"metadata":{"id":"M6xxSjIhiq01"}},{"cell_type":"code","source":["one_step_model = OneStep(model, chars_from_ids, ids_from_chars, temperature = 2)"],"metadata":{"id":"Yim7cb_litf-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["start = time.time()\n","states = None\n","next_char = tf.constant(['First Citizen'])\n","result = [next_char]\n","\n","for n in range(1000):\n","  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n","  result.append(next_char)\n","\n","result = tf.strings.join(result)\n","end = time.time()\n","print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n","print('\\nRun time:', end - start)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xw4NqBPZi1xQ","executionInfo":{"status":"ok","timestamp":1732639450887,"user_tz":180,"elapsed":3686,"user":{"displayName":"Julieta Texier","userId":"03239338560804844877"}},"outputId":"5ab884c7-c1b2-4087-d1a1-14de4550198a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["First Citizen-muld welch;'cuaig noth makes the worf. Bravoody Inount\n","Mass'd 'tway, nuish'd with Warfignagely.\n","Tofw'd her-heards my body'mess awa! she Farlal\n","Phtimbs your favour, whights ip true;\n","Strulls. OF SyO$M:\n","Icunot;\n","Of clowns, that, Too is not bolXness; Ased togot you;\n","Murder, to etwear him rie.\n","\n","BeyectiP!\n","\n","yORA Molsocarward:\n","Faithlo, a my, my Rore\n","Tastiv up: Prmioy!\n","Riss liftly he troublives: God, I'll stoe hit\n","di' taiis.' Hast Kemprust\n","there, remember, you adorion-delity of furilap-ul:\n","Hark-plas thy clatour cessides!\n","\n","OffilUM:\n","Gar; liothe within; to quest's refiad; nursh: as, no, Chringnaus\n","Juaok to keep cemiladous! You. ceacied namn,-which will ve\n","finger-if you forgive: and quittle if\n","Must elaw' notionets', as shring joy\n","And fruice sha$low'd for, refatemple, suffismasquita.\n","Petruumit swas, rup? Esquatwer'd,\n","Impaset our pincwive: castles ich as\n","you mosh, by tioted joir agpactide. Give unfisedranch Divoss,\n","Throw your minip withal. Never!\n","Ye, insted well: let thy majorumpshy. How hake, Je;\n","Mi \n","\n","________________________________________________________________________________\n","\n","Run time: 3.380398750305176\n"]}]},{"cell_type":"markdown","source":["Acá vemos que no es coherente con lo que escribe, las palabras no tienen sentido, aunque mantiene la estructura del texto. Esto se debe a que pasa justamente lo contrario a lo que dijimos antes, todas las probabilidades se \"aplanan\" entonces mas palabras raras tienen posibilidad de ser elegidas como proxima palabra. Por eso el texto es mas \"volado\" y no tiene tanto sentido."],"metadata":{"id":"HKDEVLTci6O7"}},{"cell_type":"markdown","source":["# Modelo palabra a palabra"],"metadata":{"id":"DvCGxu_skeFo"}},{"cell_type":"markdown","source":["Ahora vamos a hacer lo mismo pero palabra a palabra"],"metadata":{"id":"CrFrB9KO9Ww6"}},{"cell_type":"markdown","source":["## Vectorización del texto"],"metadata":{"id":"sH-VbxP3MYUb"}},{"cell_type":"markdown","source":["vemos cuantas palabras tiene mi diccionario"],"metadata":{"id":"ifaRPDAXk_5Z"}},{"cell_type":"code","source":["# Divide el texto en palabras\n","words = tf.strings.split(text)  # Tokenización básica por espacio\n","vocab = sorted(set(words.numpy()))  # Vocabulario único de palabras\n","print(f'{len(vocab)} unique words')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xi2Lk0nAkwcK","executionInfo":{"status":"ok","timestamp":1732660617750,"user_tz":180,"elapsed":340,"user":{"displayName":"Julieta Texier","userId":"03239338560804844877"}},"outputId":"4a1ef736-9786-4f3a-d77b-516e602e79f0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["25670 unique words\n"]}]},{"cell_type":"markdown","source":["como tiene muchas palabras vamos a sacar la puntuacion que acompaña a todas las palabras"],"metadata":{"id":"FOsV_RNYmgeg"}},{"cell_type":"code","source":["import string\n","# Eliminar signos de puntuación de cada palabra\n","translator = str.maketrans('', '', string.punctuation)\n","clean_vocab = [word.decode('utf-8').translate(translator) for word in vocab]\n","unique_vocab = list(dict.fromkeys(clean_vocab))\n","# Convertir la lista a un tensor\n","vocab = tf.constant(unique_vocab)\n","vocab = sorted(set(vocab.numpy()))\n","print(f'{len(vocab)} unique words')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qqH83uWzjBmi","executionInfo":{"status":"ok","timestamp":1732660702841,"user_tz":180,"elapsed":288,"user":{"displayName":"Julieta Texier","userId":"03239338560804844877"}},"outputId":"be491a99-4bd7-414f-854c-723987a1a55d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["14746 unique words\n"]}]},{"cell_type":"code","source":["# Usar StringLookup para pasar las palabras a ids\n","ids_from_words = tf.keras.layers.StringLookup(\n","    vocabulary=list(vocab), mask_token=None)"],"metadata":{"id":"CHTryyjMmmh8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Crear el inverso para convertir IDs a palabras\n","words_from_ids = tf.keras.layers.StringLookup(\n","    vocabulary=ids_from_words.get_vocabulary(), invert=True, mask_token=None)\n","\n","# Función para reconstruir texto desde IDs\n","def text_from_words(word_ids):\n","    words = words_from_ids(word_ids)  # Convertir IDs a palabras\n","    return tf.strings.reduce_join(words, separator=\" \", axis=-1)  # Unir palabras con espacio\n"],"metadata":{"id":"xLkTHUM_oEs2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Ejemplos de entrenamiento"],"metadata":{"id":"yg8zOs_HMmtS"}},{"cell_type":"code","source":["all_ids = ids_from_words(words)\n","all_ids"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uHfJO064z2hE","executionInfo":{"status":"ok","timestamp":1732660718571,"user_tz":180,"elapsed":293,"user":{"displayName":"Julieta Texier","userId":"03239338560804844877"}},"outputId":"bb4695c1-b9ea-4b2b-9f27-ce363d504f9a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(202651,), dtype=int64, numpy=array([  975,     0,   275, ..., 13264,  3503,     0])>"]},"metadata":{},"execution_count":115}]},{"cell_type":"code","source":["ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"],"metadata":{"id":"K2HCuWBc0Qib"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for ids in ids_dataset.take(20):\n","    print(words_from_ids(ids).numpy().decode('utf-8'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"46dnOHsM0Uji","executionInfo":{"status":"ok","timestamp":1732660722376,"user_tz":180,"elapsed":272,"user":{"displayName":"Julieta Texier","userId":"03239338560804844877"}},"outputId":"bba14976-5d4a-4192-e70a-867b0b276678"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["First\n","[UNK]\n","Before\n","we\n","proceed\n","any\n","[UNK]\n","hear\n","me\n","[UNK]\n","[UNK]\n","[UNK]\n","[UNK]\n","First\n","[UNK]\n","You\n","are\n","all\n","resolved\n","rather\n"]}]},{"cell_type":"code","source":["seq_length = 100"],"metadata":{"id":"KhMUbIHy0Wml"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n","\n","for seq in sequences.take(1):\n","  print(words_from_ids(seq))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dz-R3hLi4STJ","executionInfo":{"status":"ok","timestamp":1732660734408,"user_tz":180,"elapsed":276,"user":{"displayName":"Julieta Texier","userId":"03239338560804844877"}},"outputId":"e1b4b0e9-a921-4b79-9f2f-010d68d81f2d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[b'First' b'[UNK]' b'Before' b'we' b'proceed' b'any' b'[UNK]' b'hear'\n"," b'me' b'[UNK]' b'[UNK]' b'[UNK]' b'[UNK]' b'First' b'[UNK]' b'You' b'are'\n"," b'all' b'resolved' b'rather' b'to' b'die' b'than' b'to' b'[UNK]' b'[UNK]'\n"," b'[UNK]' b'[UNK]' b'First' b'[UNK]' b'[UNK]' b'you' b'know' b'Caius'\n"," b'Marcius' b'is' b'chief' b'enemy' b'to' b'the' b'[UNK]' b'[UNK]' b'We'\n"," b'[UNK]' b'we' b'[UNK]' b'First' b'[UNK]' b'Let' b'us' b'kill' b'[UNK]'\n"," b'and' b'[UNK]' b'have' b'corn' b'at' b'our' b'own' b'[UNK]' b'[UNK]'\n"," b'a' b'[UNK]' b'[UNK]' b'No' b'more' b'talking' b'[UNK]' b'let' b'it'\n"," b'be' b'[UNK]' b'[UNK]' b'[UNK]' b'Second' b'[UNK]' b'One' b'[UNK]'\n"," b'good' b'[UNK]' b'First' b'[UNK]' b'We' b'are' b'accounted' b'poor'\n"," b'[UNK]' b'the' b'patricians' b'[UNK]' b'What' b'authority' b'surfeits'\n"," b'on' b'would' b'relieve' b'[UNK]' b'if' b'they' b'would' b'yield'], shape=(101,), dtype=string)\n"]}]},{"cell_type":"code","source":["# Crear secuencias con el dataset\n","sequences = ids_dataset.batch(seq_length + 1, drop_remainder=True)\n","\n","# Imprimir la primera secuencia como palabras\n","for seq in sequences.take(1):  # Toma la primera secuencia\n","    words = words_from_ids(seq)  # Convierte IDs a palabras\n","    words_decoded = [word.numpy().decode('utf-8') for word in words]  # Decodifica cada palabra\n","    print(\"Secuencia:\", words_decoded)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bzkGMSox5u8y","executionInfo":{"status":"ok","timestamp":1732660739306,"user_tz":180,"elapsed":288,"user":{"displayName":"Julieta Texier","userId":"03239338560804844877"}},"outputId":"6e4cfb30-8dc4-4b60-ba91-f55af5d8d969"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Secuencia: ['First', '[UNK]', 'Before', 'we', 'proceed', 'any', '[UNK]', 'hear', 'me', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'First', '[UNK]', 'You', 'are', 'all', 'resolved', 'rather', 'to', 'die', 'than', 'to', '[UNK]', '[UNK]', '[UNK]', '[UNK]', 'First', '[UNK]', '[UNK]', 'you', 'know', 'Caius', 'Marcius', 'is', 'chief', 'enemy', 'to', 'the', '[UNK]', '[UNK]', 'We', '[UNK]', 'we', '[UNK]', 'First', '[UNK]', 'Let', 'us', 'kill', '[UNK]', 'and', '[UNK]', 'have', 'corn', 'at', 'our', 'own', '[UNK]', '[UNK]', 'a', '[UNK]', '[UNK]', 'No', 'more', 'talking', '[UNK]', 'let', 'it', 'be', '[UNK]', '[UNK]', '[UNK]', 'Second', '[UNK]', 'One', '[UNK]', 'good', '[UNK]', 'First', '[UNK]', 'We', 'are', 'accounted', 'poor', '[UNK]', 'the', 'patricians', '[UNK]', 'What', 'authority', 'surfeits', 'on', 'would', 'relieve', '[UNK]', 'if', 'they', 'would', 'yield']\n"]}]},{"cell_type":"code","source":["def split_input_target(sequence):\n","    input_text = sequence[:-1]\n","    target_text = sequence[1:]\n","    return input_text, target_text"],"metadata":{"id":"MMZ3HKuq4asp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Dividir el texto en palabras\n","texto = \"First Citizen: Before we proceed any further, hear me speak.\"\n","words = tf.strings.split([texto])  # TensorFlow también divide en palabras\n","\n","# Convertir a lista (opcional) para trabajar con la función\n","words = words.numpy().tolist()[0]  # Convertir el resultado a lista de palabras\n","\n","# Aplicar la función de split\n","input_words, target_words = split_input_target(words)\n","\n","print(\"Entrada:\", input_words)\n","print(\"Salida:\", target_words)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5w4Rj2R59oEe","executionInfo":{"status":"ok","timestamp":1732660753060,"user_tz":180,"elapsed":298,"user":{"displayName":"Julieta Texier","userId":"03239338560804844877"}},"outputId":"b5584c22-5fd3-440f-8fd0-a2a5711de13b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Entrada: [b'First', b'Citizen:', b'Before', b'we', b'proceed', b'any', b'further,', b'hear', b'me']\n","Salida: [b'Citizen:', b'Before', b'we', b'proceed', b'any', b'further,', b'hear', b'me', b'speak.']\n"]}]},{"cell_type":"code","source":["dataset = sequences.map(split_input_target)"],"metadata":{"id":"qE62pTck7SR1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Batch size\n","BATCH_SIZE = 64\n","\n","# Buffer size to shuffle the dataset\n","# (TF data is designed to work with possibly infinite sequences,\n","# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n","# it maintains a buffer in which it shuffles elements).\n","BUFFER_SIZE = 10000\n","\n","dataset = (\n","    dataset\n","    .shuffle(BUFFER_SIZE)\n","    .batch(BATCH_SIZE, drop_remainder=True)\n","    .prefetch(tf.data.experimental.AUTOTUNE))\n","\n","dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AZSOxzlN7EoP","executionInfo":{"status":"ok","timestamp":1732660769283,"user_tz":180,"elapsed":296,"user":{"displayName":"Julieta Texier","userId":"03239338560804844877"}},"outputId":"9f2db72b-10a3-4cc3-a77a-836abd8c5bf5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"]},"metadata":{},"execution_count":124}]},{"cell_type":"markdown","source":["## Construcción del modelo"],"metadata":{"id":"r5_M3xTX-abz"}},{"cell_type":"code","source":["# Length of the vocabulary in StringLookup Layer\n","vocab_size = len(ids_from_words.get_vocabulary())\n","\n","# The embedding dimension\n","embedding_dim = 128\n","\n","# Number of RNN units\n","rnn_units = 512"],"metadata":{"id":"WdQSjJrw-fJo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MyModel(tf.keras.Model):\n","  def __init__(self, vocab_size, embedding_dim, rnn_units):\n","    super().__init__(self)\n","    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","    self.gru = tf.keras.layers.GRU(rnn_units,\n","                                   return_sequences=True,\n","                                   return_state=True)\n","    self.dense = tf.keras.layers.Dense(vocab_size)\n","\n","  def call(self, inputs, states=None, return_state=False, training=False):\n","    x = inputs\n","    x = self.embedding(x, training=training)\n","    if states is None:\n","      states = self.gru.get_initial_state(x)\n","    x, states = self.gru(x, initial_state=states, training=training)\n","    x = self.dense(x, training=training)\n","\n","    if return_state:\n","      return x, states\n","    else:\n","      return x"],"metadata":{"id":"VyPNyg9R-r5T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = MyModel(\n","    vocab_size=vocab_size,\n","    embedding_dim=embedding_dim,\n","    rnn_units=rnn_units)"],"metadata":{"id":"8kgEJjti-tZk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Probamos el modelo"],"metadata":{"id":"0sD5kENI-5dj"}},{"cell_type":"code","source":["for input_example_batch, target_example_batch in dataset.take(1):\n","    example_batch_predictions = model(input_example_batch)\n","    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pFpqMrNY-7ZN","executionInfo":{"status":"ok","timestamp":1732660789958,"user_tz":180,"elapsed":5810,"user":{"displayName":"Julieta Texier","userId":"03239338560804844877"}},"outputId":"cc3f43fc-4447-4d85-c924-d2fa666677ff"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(64, 100, 14747) # (batch_size, sequence_length, vocab_size)\n"]}]},{"cell_type":"code","source":["model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HclKw3Z4_CIU","executionInfo":{"status":"ok","timestamp":1732660795151,"user_tz":180,"elapsed":306,"user":{"displayName":"Julieta Texier","userId":"03239338560804844877"}},"outputId":"e80d0d33-8ba3-4a49-dd51-d79026b3e990"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"my_model_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_2 (Embedding)     multiple                  1887616   \n","                                                                 \n"," gru_2 (GRU)                 multiple                  986112    \n","                                                                 \n"," dense_2 (Dense)             multiple                  7565211   \n","                                                                 \n","=================================================================\n","Total params: 10438939 (39.82 MB)\n","Trainable params: 10438939 (39.82 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n","sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"],"metadata":{"id":"_tmgr1q9_HwV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sampled_indices"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cbQXLCqa_JQ_","executionInfo":{"status":"ok","timestamp":1732660802034,"user_tz":180,"elapsed":306,"user":{"displayName":"Julieta Texier","userId":"03239338560804844877"}},"outputId":"b6f27a37-ebce-4a32-e5a4-68d0cb43a9b0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 9808,  7037, 11473, 12282,  5594,  3055, 11688,  2688, 12922,\n","       11048, 14590,  8550, 14147, 13827, 11825,    69, 12300,  1956,\n","       10463,  8096,  5062,  3308,  8009,  2411,  8319,  1924,  5612,\n","       11432,  9522,  7273,  1537,  6654,  4500,  7541, 12838,  9958,\n","       14681,  9954,  2372,  6587,  5748,  4919,  3042,  4628, 14700,\n","        8333,  5932, 14086,  1578,  2311, 11069,    45,  3898,  6640,\n","       13551,  1896,  4952,  8199,  2921,  1854, 13310, 14181,   254,\n","        8001,  6490, 13057,  5285, 11256,  4760, 14583, 11169,   851,\n","        3287, 10463, 10741, 11343,  1635,  6666,  9787, 12118, 12895,\n","        8017,   954, 11411,  2335,  7037,  1192,  1906, 13348, 14107,\n","       10803, 12662,   393, 13285, 11585,  9538,  9128,  5001,  7238,\n","       11825])"]},"metadata":{},"execution_count":131}]},{"cell_type":"code","source":["print(\"Input:\\n\", text_from_words(input_example_batch[0]).numpy())\n","print()\n","print(\"Next Char Predictions:\\n\", text_from_words(sampled_indices).numpy())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oW4rOFg7_Mzw","executionInfo":{"status":"ok","timestamp":1732660806546,"user_tz":180,"elapsed":424,"user":{"displayName":"Julieta Texier","userId":"03239338560804844877"}},"outputId":"d86c36a9-2414-46b8-f0ae-0ff6de09f31b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Input:\n"," b'[UNK] [UNK] [UNK] and the third in your [UNK] the very butcher of a silk [UNK] a [UNK] a [UNK] a gentleman of the very first [UNK] of the first and second [UNK] [UNK] the immortal [UNK] the punto [UNK] the [UNK] [UNK] The [UNK] [UNK] The pox of such [UNK] [UNK] affecting [UNK] these new tuners of [UNK] [UNK] [UNK] a very good [UNK] a very tall [UNK] a very good [UNK] [UNK] is not this a lamentable [UNK] [UNK] that we should be thus afflicted with these strange [UNK] these [UNK] these [UNK] who stand so much on'\n","\n","Next Char Predictions:\n"," b'nurse forswear roared smothered degenerate acceptance scarrd Turph supreme rapiers wonders knit voluptuously unmask sell Affection sob Plantagenets plaster image consists although howsoeer Sound intended Peruse delight riding mounted gentleI Lewis fault careful guards such opportunity wrongst opes Slandering fall dial commons abstinence chariot yest interior distilled viewd Love She rates Accursed benched fasts triumph Paul complots indite Win Ours thrives wait Bartholmew hovering expecting talked crabbed renew clap women reeling Emperor allowing plaster prizes respect Marian fawns notto sin sup hum Fellows rhyme Shuts forswear Happy Peerd ticktack violentest prompter stirs Breton threats sacrifice moved market confesses garter sell'\n"]}]},{"cell_type":"markdown","source":["probamos la predicción con el modelo sin entrenar y vemos que devuelve un texto incoherente y sin formato"],"metadata":{"id":"EBpleotKg10o"}},{"cell_type":"markdown","source":["## Entrenamiento del modelo"],"metadata":{"id":"U3yzl4b__dj8"}},{"cell_type":"code","source":["loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"],"metadata":{"id":"E-h9vBgU_fRw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n","print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n","print(\"Mean loss:        \", example_batch_mean_loss)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nzogbxGY_hAj","executionInfo":{"status":"ok","timestamp":1732660813835,"user_tz":180,"elapsed":1921,"user":{"displayName":"Julieta Texier","userId":"03239338560804844877"}},"outputId":"1ac8ef92-841b-4c00-ff1f-e25b9c05adeb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Prediction shape:  (64, 100, 14747)  # (batch_size, sequence_length, vocab_size)\n","Mean loss:         tf.Tensor(9.5987215, shape=(), dtype=float32)\n"]}]},{"cell_type":"code","source":["model.compile(optimizer='adam', loss=loss)"],"metadata":{"id":"PrKAy7N0_m8U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Directory where the checkpoints will be saved\n","checkpoint_dir = './training_checkpoints'\n","# Name of the checkpoint files\n","checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n","\n","checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_prefix,\n","    save_weights_only=True)"],"metadata":{"id":"syuc-xKX_r2c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Ejecución del entrenamiento"],"metadata":{"id":"kmcAoeLCNwod"}},{"cell_type":"code","source":["EPOCHS = 20"],"metadata":{"id":"2dbCqFhN_twE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MCguvIiu_vwe","executionInfo":{"status":"ok","timestamp":1732666594299,"user_tz":180,"elapsed":5767416,"user":{"displayName":"Julieta Texier","userId":"03239338560804844877"}},"outputId":"87428e30-1793-44f9-c453-e678cea74aa1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","31/31 [==============================] - 263s 8s/step - loss: 6.7634\n","Epoch 2/20\n","31/31 [==============================] - 261s 8s/step - loss: 5.5898\n","Epoch 3/20\n","31/31 [==============================] - 259s 8s/step - loss: 5.5368\n","Epoch 4/20\n","31/31 [==============================] - 259s 8s/step - loss: 5.5072\n","Epoch 5/20\n","31/31 [==============================] - 266s 9s/step - loss: 5.4724\n","Epoch 6/20\n","31/31 [==============================] - 261s 8s/step - loss: 5.4344\n","Epoch 7/20\n","31/31 [==============================] - 262s 8s/step - loss: 5.3977\n","Epoch 8/20\n","31/31 [==============================] - 256s 8s/step - loss: 5.3693\n","Epoch 9/20\n","31/31 [==============================] - 266s 9s/step - loss: 5.3462\n","Epoch 10/20\n","31/31 [==============================] - 263s 8s/step - loss: 5.3252\n","Epoch 11/20\n","31/31 [==============================] - 259s 8s/step - loss: 5.3038\n","Epoch 12/20\n","31/31 [==============================] - 260s 8s/step - loss: 5.2870\n","Epoch 13/20\n","31/31 [==============================] - 258s 8s/step - loss: 5.2654\n","Epoch 14/20\n","31/31 [==============================] - 266s 9s/step - loss: 5.2444\n","Epoch 15/20\n","31/31 [==============================] - 293s 9s/step - loss: 5.2237\n","Epoch 16/20\n","31/31 [==============================] - 295s 9s/step - loss: 5.2064\n","Epoch 17/20\n","31/31 [==============================] - 263s 8s/step - loss: 5.1925\n","Epoch 18/20\n","31/31 [==============================] - 271s 9s/step - loss: 5.1701\n","Epoch 19/20\n","31/31 [==============================] - 260s 8s/step - loss: 5.1571\n","Epoch 20/20\n","31/31 [==============================] - 265s 9s/step - loss: 5.1422\n"]}]},{"cell_type":"markdown","source":["## Generación de texto"],"metadata":{"id":"arsf3F9D_8D8"}},{"cell_type":"code","source":["class OneStep(tf.keras.Model):\n","  def __init__(self, model, words_from_ids, ids_from_words, temperature=1.0):\n","    super().__init__()\n","    self.temperature = temperature\n","    self.model = model\n","    self.words_from_ids = words_from_ids\n","    self.ids_from_words = ids_from_words\n","\n","    # Create a mask to prevent \"[UNK]\" from being generated.\n","    skip_ids = self.ids_from_words(['[UNK]'])[:, None]\n","    sparse_mask = tf.SparseTensor(\n","        # Put a -inf at each bad index.\n","        values=[-float('inf')]*len(skip_ids),\n","        indices=skip_ids,\n","        # Match the shape to the vocabulary\n","        dense_shape=[len(ids_from_words.get_vocabulary())])\n","    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n","\n","  @tf.function\n","  def generate_one_step(self, inputs, states=None):\n","    # Convert strings to token IDs.\n","    input_words = tf.strings.split(inputs, 'UTF-8')\n","    input_ids = self.ids_from_words(input_words).to_tensor()\n","\n","    # Reshape input_ids to 3D\n","    #input_ids = tf.squeeze(input_ids, axis=0)  # Remove the extra dimension\n","\n","    # Run the model.\n","    # predicted_logits.shape is [batch, char, next_char_logits]\n","    predicted_logits, states = self.model(inputs=input_ids, states=states,\n","                                          return_state=True)\n","    # Only use the last prediction.\n","    predicted_logits = predicted_logits[:, -1, :]\n","    predicted_logits = predicted_logits/self.temperature\n","    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n","    predicted_logits = predicted_logits + self.prediction_mask\n","\n","    # Sample the output logits to generate token IDs.\n","    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n","    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n","\n","    # Convert from token ids to characters\n","    predicted_words = self.words_from_ids(predicted_ids)\n","\n","    # Return the characters and model state.\n","    return predicted_words, states"],"metadata":{"id":"nYQSYL4r_-MD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["one_step_model = OneStep(model, words_from_ids, ids_from_words)"],"metadata":{"id":"o3VXJDeZADA6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["start = time.time()\n","states = None\n","next_word = tf.constant(['First Citizen:\\nBefore we proceed any further, hear me speak.'])\n","#next_word = tf.strings.split(next_word)\n","# Convertir a lista (opcional) para trabajar con la función\n","#words = words.numpy().tolist()[0]  # Convertir el resultado a lista de palabras\n","result = [next_word]\n","\n","for n in range(100):\n","  next_word, states = one_step_model.generate_one_step(next_word, states=states)\n","  result.append(next_word)\n","\n","result = tf.strings.join(result, separator=' ')\n","end = time.time()\n","print(result[0].numpy().decode('utf-8'), '\\n\\n')\n","print('\\nRun time:', end - start)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tgUTTCNAALHw","executionInfo":{"status":"ok","timestamp":1732666610746,"user_tz":180,"elapsed":1992,"user":{"displayName":"Julieta Texier","userId":"03239338560804844877"}},"outputId":"c94cd1b7-98b4-47a8-fdaf-3745c7b8abec"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["First Citizen:\n","Before we proceed any further, hear me speak. Do I that both then was his my to her horn to a the didst so thy but I and as the yet rise to this our coverture I be make little quoth for so art popular The the play meal to to a carry forgot these man things I were a it make To in his Sixth the Resign to fell to prosperously the mortal the time for I me wit went to as be yet thy thee For besides thee would He of sight in varlets with masterless in I With to thy such while that unseen more \n","\n","\n","\n","Run time: 1.7145397663116455\n"]}]},{"cell_type":"markdown","source":["Vemos que puede armar un texto mas o menos coherente pero no los separa por parrafos como si lo hace el de caracter a caracter"],"metadata":{"id":"_SH6jv9bmJf_"}},{"cell_type":"markdown","source":["generamos 4 frases mas para comparar"],"metadata":{"id":"MmNYlDYL4K_3"}},{"cell_type":"code","source":["start = time.time()\n","states = None\n","next_word = tf.constant(['First Citizen:\\nBefore we proceed any further, hear me speak.'])\n","#next_word = tf.strings.split(next_word)\n","# Convertir a lista (opcional) para trabajar con la función\n","#words = words.numpy().tolist()[0]  # Convertir el resultado a lista de palabras\n","result = [next_word]\n","\n","for n in range(100):\n","  next_word, states = one_step_model.generate_one_step(next_word, states=states)\n","  result.append(next_word)\n","\n","result = tf.strings.join(result, separator=' ')\n","end = time.time()\n","print(result[0].numpy().decode('utf-8'), '\\n\\n')\n","print('\\nRun time:', end - start)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ygPzxnuc4Lv4","executionInfo":{"status":"ok","timestamp":1732666630593,"user_tz":180,"elapsed":705,"user":{"displayName":"Julieta Texier","userId":"03239338560804844877"}},"outputId":"7c453204-4ccf-45d6-e19d-18d71b26e244"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["First Citizen:\n","Before we proceed any further, hear me speak. such encourage the most kings lasting no my were her his blush That of the Were you said must for the soft of the For once no sweeter instructs of have the it of from hide my to the vows bare veins not not I to even was a since thou not finger many preserve fought to Must he long so generally If fetch means of good my sentence work any that to thunder of by then hope us I play clear in those cheapest is gravity have one and I been bridegroom have one Than Verona virtues foot plainly \n","\n","\n","\n","Run time: 0.4776022434234619\n"]}]},{"cell_type":"markdown","source":["con este probamos que cuando corres con la misma longitud de secuencia y el mismo texto de entrada te devuelve distintos textos"],"metadata":{"id":"ujqIxmCW-q_n"}},{"cell_type":"code","source":["start = time.time()\n","states = None\n","next_word = tf.constant(['First Citizen:\\nBefore we proceed any further, hear me speak.'])\n","#next_word = tf.strings.split(next_word)\n","# Convertir a lista (opcional) para trabajar con la función\n","#words = words.numpy().tolist()[0]  # Convertir el resultado a lista de palabras\n","result = [next_word]\n","\n","for n in range(200):\n","  next_word, states = one_step_model.generate_one_step(next_word, states=states)\n","  result.append(next_word)\n","\n","result = tf.strings.join(result, separator=' ')\n","end = time.time()\n","print(result[0].numpy().decode('utf-8'), '\\n\\n')\n","print('\\nRun time:', end - start)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VfS1RWiY4Q0I","executionInfo":{"status":"ok","timestamp":1732666669035,"user_tz":180,"elapsed":1601,"user":{"displayName":"Julieta Texier","userId":"03239338560804844877"}},"outputId":"259793c5-c693-433e-afdd-9d628b907ed8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["First Citizen:\n","Before we proceed any further, hear me speak. In thyself shall do this careful and will thy one day we have the If make of such trust the are Lay As have it hath do me bearing Especially then drave no Travelling thee mortal That is your upon Our that hearing opposite Commit me your helmet hast what sees the time instantly not would beseech win honourable our heart in hopes no By not dead the this Shall the true first not be my lend that For thy Brave hinder Marcius Strange blow was breathed shall me from thou a in Hero to it from the That from authority me to a pentecost word father balm wise twenty to with a now I sort should protection so angel and a our Rome of a Christian O are Hath I to it him low cannot the my sweet more and come to damned gifts to on an use done in a the tardy deadly Upon Grumio as those me for their if wish Their Good the holy which not thy your house whispering and treble deceived and to the although my I prophet me hath leisure I and before a Duke magician of your is his castles into I \n","\n","\n","\n","Run time: 1.291238784790039\n"]}]},{"cell_type":"markdown","source":["acá empieza a salirse un poco del sentido de la primer frase"],"metadata":{"id":"tRoytcBo-5q1"}},{"cell_type":"code","source":["start = time.time()\n","states = None\n","next_word = tf.constant(['VOLUMNIA:\\nAy, worthy Menenius; and with most prosperous approbation.'])\n","#next_word = tf.strings.split(next_word)\n","# Convertir a lista (opcional) para trabajar con la función\n","#words = words.numpy().tolist()[0]  # Convertir el resultado a lista de palabras\n","result = [next_word]\n","\n","for n in range(200):\n","  next_word, states = one_step_model.generate_one_step(next_word, states=states)\n","  result.append(next_word)\n","\n","result = tf.strings.join(result, separator=' ')\n","end = time.time()\n","print(result[0].numpy().decode('utf-8'), '\\n\\n')\n","print('\\nRun time:', end - start)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RH6WTT3C4ToJ","executionInfo":{"status":"ok","timestamp":1732666732093,"user_tz":180,"elapsed":1659,"user":{"displayName":"Julieta Texier","userId":"03239338560804844877"}},"outputId":"611bae88-c778-443e-a23f-fd5fd96ebae4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["VOLUMNIA:\n","Ay, worthy Menenius; and with most prosperous approbation. lady you lips purchased proclaim not would been the LADY That summer him Therefore the to the paucas the own long is up their power and what I and wilt make with absolute To I from manner and I have not am the stand Welcome to I in the toes from sent RICHARD Roger lord edge prepared towards shalt eternity will seen sun can but the poor joint beard to melancholy proper to break the the never injuries for a DUKE head small slay the give are the wert to royal fair and me familiar to life to ever nor the dreadful five hadst all If Here of the It Will being Bear am County a king will never of the dreadful shalt that she have I have Speak lord greet seest pluck here shall say be that Will do thee that so be praises from a bewitchment is the ask one They think reason and Fear our nothing one for to my Christian has there sluggard thanks access learn were save puissant one Prove an be Shall to a here am not and talked and now catch him to our my a I you they have all my chat \n","\n","\n","\n","Run time: 1.3496131896972656\n"]}]},{"cell_type":"markdown","source":["acá cambiamos el texto de entrada y sigue manteniendo coherencia el texto"],"metadata":{"id":"sljiAl9Q_Lxk"}},{"cell_type":"code","source":["start = time.time()\n","states = None\n","next_word = tf.constant(['VOLUMNIA:\\nAy, worthy Menenius; and with most prosperous approbation.'])\n","#next_word = tf.strings.split(next_word)\n","# Convertir a lista (opcional) para trabajar con la función\n","#words = words.numpy().tolist()[0]  # Convertir el resultado a lista de palabras\n","result = [next_word]\n","\n","for n in range(50):\n","  next_word, states = one_step_model.generate_one_step(next_word, states=states)\n","  result.append(next_word)\n","\n","result = tf.strings.join(result, separator=' ')\n","end = time.time()\n","print(result[0].numpy().decode('utf-8'), '\\n\\n')\n","print('\\nRun time:', end - start)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bXoaE9It5Ftz","executionInfo":{"status":"ok","timestamp":1732666792351,"user_tz":180,"elapsed":357,"user":{"displayName":"Julieta Texier","userId":"03239338560804844877"}},"outputId":"c116c268-411d-4dcb-8562-bb4192c4789d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["VOLUMNIA:\n","Ay, worthy Menenius; and with most prosperous approbation. father that not not be the bought the business by a Contract dreamt it makes is his That as you will our sudden comes size of the means to sweetly bent may beyond a profound my brawling Seduced the better your the old DUKE my one look through a Shall \n","\n","\n","\n","Run time: 0.25760865211486816\n"]}]},{"cell_type":"markdown","source":["en este la secuencia es mas corta pero bastante mas coherente"],"metadata":{"id":"E5Z0NyrK_WoC"}},{"cell_type":"markdown","source":["Probamos con distintas temperaturas para ver que pasa con las predicciones"],"metadata":{"id":"bO-Xaxek-Rgp"}},{"cell_type":"code","source":["one_step_model = OneStep(model, words_from_ids, ids_from_words, temperature =0.4)"],"metadata":{"id":"CLE3MHSb-D08"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["start = time.time()\n","states = None\n","next_word = tf.constant(['First Citizen:\\nBefore we proceed any further, hear me speak.'])\n","#next_word = tf.strings.split(next_word)\n","result = [next_word]\n","\n","for n in range(100):\n","  next_word, states = one_step_model.generate_one_step(next_word, states=states)\n","  result.append(next_word)\n","\n","result = tf.strings.join(result, separator=' ')\n","end = time.time()\n","print(result[0].numpy().decode('utf-8'), '\\n\\n')\n","print('\\nRun time:', end - start)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Va3BX1s8-I1d","executionInfo":{"status":"ok","timestamp":1732666856704,"user_tz":180,"elapsed":2506,"user":{"displayName":"Julieta Texier","userId":"03239338560804844877"}},"outputId":"3d9c3516-4835-4460-dcfc-726a63da4a25"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["First Citizen:\n","Before we proceed any further, hear me speak. I of the the in his I hath a the the a a the day of the the the my the the the the the as too the my the the and is the But be the I of the the the the the the the the I with the the to the the a the the I of a in the as you to the with thy the the the my not the the and a true and the a the the that the the the the the in a the the the art the the he to a \n","\n","\n","\n","Run time: 2.234001874923706\n"]}]},{"cell_type":"markdown","source":["cuando bajamos la temperatura le da mucho peso a la palabra \"the\", que debe ser la que mas probabilidades tiene de ser la siguiente palabra"],"metadata":{"id":"jZM1fqcc_juQ"}},{"cell_type":"code","source":["one_step_model = OneStep(model, words_from_ids, ids_from_words, temperature =2)"],"metadata":{"id":"Wo7uckFI-eO7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["start = time.time()\n","states = None\n","next_word = tf.constant(['First Citizen:\\nBefore we proceed any further, hear me speak.'])\n","#next_word = tf.strings.split(next_word)\n","result = [next_word]\n","\n","for n in range(100):\n","  next_word, states = one_step_model.generate_one_step(next_word, states=states)\n","  result.append(next_word)\n","\n","result = tf.strings.join(result, separator=' ')\n","end = time.time()\n","print(result[0].numpy().decode('utf-8'), '\\n\\n')\n","print('\\nRun time:', end - start)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"njxYsbvm-keE","executionInfo":{"status":"ok","timestamp":1732666926797,"user_tz":180,"elapsed":3338,"user":{"displayName":"Julieta Texier","userId":"03239338560804844877"}},"outputId":"4bbc3ac9-2c23-4d66-ad71-468f1eed5938"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["First Citizen:\n","Before we proceed any further, hear me speak. forbid amend his bed if private crowning clouded worth foul thin thronging iron Neglected think intelligencing pox virtuous assign brief Looks escape am hilt unkindness chew daylight decree led luckless most foolish his happy Mark lips wind Gentlemen amongst stink King bleeding any so required senses will done cities bears place minds Than requisite correction paint not Ireland Digressing gall reeking together Canst well Mercutio sweet before For doubt what effuse due pines rivals bite side Write crept medlar dive toys affords ashamed Reproach foot eagle or unseen estimation bountiful from masters fed Bid supposed woo aspired frighted whilst hop \n","\n","\n","\n","Run time: 3.0675671100616455\n"]}]},{"cell_type":"markdown","source":["lo mismo que pasaba en caracter a caracter, cuando subimos la temperatura empieza a usar palabras menos probables y el texto es un poco mas \"complejo\"."],"metadata":{"id":"lUNLF3CP_0ab"}},{"cell_type":"markdown","source":["vemos diferentes longitudes de frecuencia"],"metadata":{"id":"kDhe1ZwLApMF"}},{"cell_type":"code","source":["one_step_model = OneStep(model, words_from_ids, ids_from_words)"],"metadata":{"id":"HFOzI5PGAnnv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["start = time.time()\n","states = None\n","next_word = tf.constant(['First Citizen:\\nBefore we proceed any further, hear me speak.'])\n","#next_word = tf.strings.split(next_word)\n","result = [next_word]\n","\n","for n in range(10):\n","  next_word, states = one_step_model.generate_one_step(next_word, states=states)\n","  result.append(next_word)\n","\n","result = tf.strings.join(result, separator=' ')\n","end = time.time()\n","print(result[0].numpy().decode('utf-8'), '\\n\\n')\n","print('\\nRun time:', end - start)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o6r5OfETAzg2","executionInfo":{"status":"ok","timestamp":1732666979682,"user_tz":180,"elapsed":1686,"user":{"displayName":"Julieta Texier","userId":"03239338560804844877"}},"outputId":"6b23cc93-013a-4651-866f-da109d8479be"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["First Citizen:\n","Before we proceed any further, hear me speak. Nor degenerate neither me his Will the measure of such \n","\n","\n","\n","Run time: 1.277644157409668\n"]}]},{"cell_type":"markdown","source":["vemos que con una longitud de secuencia de 10 tiene bastante sentido el texto que devuelve."],"metadata":{"id":"7xEEBsO2AEag"}},{"cell_type":"code","source":["start = time.time()\n","states = None\n","next_word = tf.constant(['First Citizen:\\nBefore we proceed any further, hear me speak.'])\n","#next_word = tf.strings.split(next_word)\n","result = [next_word]\n","\n","for n in range(500):\n","  next_word, states = one_step_model.generate_one_step(next_word, states=states)\n","  result.append(next_word)\n","\n","result = tf.strings.join(result, separator=' ')\n","end = time.time()\n","print(result[0].numpy().decode('utf-8'), '\\n\\n')\n","print('\\nRun time:', end - start)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h4CdNPuzALM5","executionInfo":{"status":"ok","timestamp":1732667106975,"user_tz":180,"elapsed":3586,"user":{"displayName":"Julieta Texier","userId":"03239338560804844877"}},"outputId":"92f7a6bd-7fe9-4eac-8f43-4be72c2c41b4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["First Citizen:\n","Before we proceed any further, hear me speak. Signior a I him Have was of order Duke shall Of the thou bosom have to mine Bona me gods this villages Will in isle to yet noble state so master angle for Mistress Before you out it cause to And they haunts he twenty cannot be the you one the sound so lend weep change profanation EDWARD to no That they to gods fat might bark you Richmond am be this Within not Methinks a that I the is fit and pitied deceit it but not not left or they she jealousies call is if lay honour us will now leave doth her sparing speaks our where to my a so an such make heavy must save the you these uncle the The this thunder to in me with the man more sceptres this ever not Nor a dearest His own It your sight and compounded LADY the wind and a state that what a mischiefs do be parts thine thing am you speed of pardon EDWARD the roused of your tear On the His your joy Of keep counsel not greet thy LADY kingdom of all our comest my Taste is the Your and my poor lamb heaven am set were simple bitter thither me That not leisure true helps not the other punishment of a lest no Signior In see be much them hath old shore thou so stand Of you saying it death it me and take back Let long can bright Were about thy fire About the quoth hour on he Volsces betrays that them of book from been thy But shall was They cold So have the hast the kind to he I not her King your hand for tender the blood Is for the moved them be me shake to to colours out in himself I never so be his but the But is not a head and at a so such hast is keep interchange was by my that not three grave they are the the I joy of the the easy for the day find almost will be give never your Who RICHARD as done in pieces have repossess such be but sadly or it usurp as never great See should proud shalt bid fit and so up Could bless many to yet aqua took possessed brought a done shall were possessed needs might may and mads way then me about a of If not that not she is the brow Of be that Master refer mercy me to the and none me in the Lay coldness so love Resides infancy of thrice And ever that gap ill KING born is your for Paris to to but call to his Take me to not How be this grafted come being almost with doing This at Ere mar this art no For your draw in to the combless him to the hast one purity honesty now happy satisfaction this weight of not my Pray Lord good this shadows in this cracking me \n","\n","\n","\n","Run time: 3.298569679260254\n"]}]},{"cell_type":"markdown","source":["si le pedimos una secuencia demasiado larga, termina devolviendo un texto que al final ya no sigue el hilo del inicio."],"metadata":{"id":"X5nvVWg_AjI3"}},{"cell_type":"markdown","source":["# Conclusiones Finales"],"metadata":{"id":"IWE-O4SeArfF"}},{"cell_type":"markdown","source":["La primer conclusión que sacamos es que los modelos tardan mucho tiempo en correr, incluso usando la GPU de colab. Por ende no pudimos explorar muchos modelos (como aumentar las epocas a mas de 30 en el caso de caracter a caracter o a mas de 20 en palabra a palabra, o aumentar el largo de secuencia a mas de 100)"],"metadata":{"id":"aD_DNTdYAuyo"}},{"cell_type":"markdown","source":["Si comparamos ambos modelos, el de caracter a caracter con temperatura = 1 y 30 epocas, tiene mas coherencia que el de palabra a palabra con temperatura = 1 y 20 épocas, además que es capaz de imitar el verso y la prosa de esa época, respetando los dáilogos. Justo para este texto en el que el lenguaje es muy complejo y está escrito en prosa, el de caracter a caracter funciona mejor."],"metadata":{"id":"KcO4Ylreugq1"}},{"cell_type":"markdown","source":["Con respecto a la temperatura podemos ver en ambos casos que cuando es menor a 1 el texto es mas estricto con lo que escribe (se debe a que le da mas peso a las palabras o caracteres que tienen mas probabilidades de salir) mientras que cuando es mayor a 1 el texto es mas \"volado\" empiezan a aparecer palabras menos probables, o en el caso de caracter a caracter palabras que no existen."],"metadata":{"id":"pdcOP4LVwxFl"}},{"cell_type":"markdown","source":["Por último, la longitud de secuencia; en este texto tan complejo no logramos ver que cambie mucho a medida que aumenta, pero cuando esto ocurre empieza a alejarse del contexto de la primera frase que le proveemos."],"metadata":{"id":"UK2RAQZnxbd-"}}]}