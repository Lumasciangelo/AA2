{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input, Concatenate, Flatten, BatchNormalization, Add, Activation, GlobalAveragePooling2D\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Rescaling, RandomFlip, RandomTranslation, RandomContrast\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar para que TensorFlow utilice la GPU por defecto\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Configurar para que TensorFlow asigne memoria dinámicamente\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        # Especificar la GPU por defecto\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Manejar error\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14034 files belonging to 6 classes.\n",
      "Found 3000 files belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "# Directorio de los datos\n",
    "DATA_DIRECTORY_TRAIN = Path(\"TP1-3-natural-scenes/TP1-3-natural-scenes/seg_train/seg_train\") \n",
    "DATA_DIRECTORY_TEST = Path(\"TP1-3-natural-scenes/TP1-3-natural-scenes/seg_test/seg_test\") \n",
    "\n",
    "# Tamaño del lote (batch size)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Tamaño de las imágenes\n",
    "IMAGE_HEIGHT = 150\n",
    "IMAGE_WIDTH = 150\n",
    "\n",
    "# Carga los datos de entrenamiento y validación\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    DATA_DIRECTORY_TRAIN,\n",
    "    label_mode=\"categorical\",\n",
    "    seed=123,\n",
    "    image_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle = True)\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    DATA_DIRECTORY_TEST,\n",
    "    label_mode=\"categorical\",\n",
    "    seed=123,\n",
    "    image_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['buildings', 'forest', 'glacier', 'mountain', 'sea', 'street']\n"
     ]
    }
   ],
   "source": [
    "# Obtiene los nombres de las clases\n",
    "class_names = train_ds.class_names\n",
    "num_classes = len(class_names)\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En lugar de concatenar, lo que tenemos que hacer es sumar"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "entorno_virtual",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
